{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import xml.etree\n",
    "import numpy as np\n",
    "import mrcnn.utils\n",
    "import mrcnn.config\n",
    "import mrcnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableBankDataset(mrcnn.utils.Dataset):\n",
    "\n",
    "    def load_dataset(self, dataset_dir, is_train=True):\n",
    "        # Adds information (image ID, image path, and annotation file path) about each image in a dictionary.\n",
    "        self.add_class(\"dataset\", 1, \"table\")\n",
    "\n",
    "        images_dir = dataset_dir + '/images/'\n",
    "        annotations_dir = dataset_dir + '/annotations/'\n",
    "\n",
    "        for filename in os.listdir(images_dir):\n",
    "            image_id = filename[:-4]\n",
    "\n",
    "            img_path = images_dir + filename\n",
    "            ann_path = annotations_dir + image_id + '.xml'\n",
    "\n",
    "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
    "\n",
    "    # Loads the binary masks for an image.\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        path = info['annotation']\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        masks = np.zeros([h, w, len(boxes)], dtype='uint8')\n",
    "\n",
    "        class_ids = list()\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "            class_ids.append(self.class_names.index('table'))\n",
    "        return masks, np.asarray(class_ids, dtype='int32')\n",
    "\n",
    "    # A helper method to extract the bounding boxes from the annotation file\n",
    "    def extract_boxes(self, filename):\n",
    "        tree = xml.etree.ElementTree.parse(filename)\n",
    "\n",
    "        root = tree.getroot()\n",
    "\n",
    "        boxes = list()\n",
    "        for box in root.findall('.//bndbox'):\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(coors)\n",
    "\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        return boxes, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'segmentation': [[102, 601, 102, 672, 251, 672, 251, 601]], 'area': 10579, 'image_id': 15, 'category_id': 1, 'id': 19, 'iscrowd': 0, 'bbox': [102, 601, 149, 71], 'width': 596, 'height': 842}\n"
     ]
    }
   ],
   "source": [
    "dataset_dir='C:/Users/dsash/Repository/table/TableBank/Detection' \n",
    "annotations = json.load(open(dataset_dir + '/annotations/' + 'tablebank_latex_train.json'))\n",
    "print(type(annotations))\n",
    "# Заданный id для поиска\n",
    "target_id = 19\n",
    "\n",
    "for i, image in enumerate(annotations[\"images\"]):\n",
    "    if image[\"id\"] == target_id:\n",
    "        for annotation in annotations[\"annotations\"]:\n",
    "            if image[\"id\"] == annotation[\"id\"]:\n",
    "                annotation[\"width\"] = image[\"width\"]\n",
    "                annotation[\"height\"] = image[\"height\"]\n",
    "                print(annotation)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableBankDataset(mrcnn.utils.Dataset):\n",
    "\n",
    "    def load_dataset(self, dataset_dir, mode='train'):\n",
    "        # Adds information (image ID, image path, and annotation file path) about each image in a dictionary.\n",
    "        self.add_class(\"dataset\", 1, \"table\")\n",
    "        \n",
    "        images_dir = dataset_dir + '/images/'\n",
    "        annotations_dir = dataset_dir + '/annotations/'\n",
    "\n",
    "        annotations = json.load(open(dataset_dir + '/annotations/' + 'tablebank_latex_train.json'))\n",
    "\n",
    "        for i, image in enumerate(annotations[\"images\"]):\n",
    "            if i > 1000:\n",
    "                break\n",
    "            for annotation in annotations[\"annotations\"]:\n",
    "                if image[\"id\"] == annotation[\"id\"]:\n",
    "                    img_path = images_dir + image[\"file_name\"]\n",
    "                    img_info = annotation\n",
    "                    img_info[\"width\"] = image[\"width\"]\n",
    "                    img_info[\"height\"] = image[\"height\"]\n",
    "                    self.add_image('dataset', image_id=image[\"id\"], path=img_path, annotation=img_info)\n",
    "                    break\n",
    "    \n",
    "    # A helper method to extract the bounding boxes from the annotation file\n",
    "    def extract_boxes(self, filename):\n",
    "        img_info = filename\n",
    "        boxes = []\n",
    "        boxes.append(img_info[\"bbox\"])\n",
    "        width = img_info[\"width\"]\n",
    "        height = img_info[\"height\"]\n",
    "        return boxes, width, height\n",
    "\n",
    "    # Loads the binary masks for an image.\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        path = info['annotation']\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        masks = np.zeros([h, w, len(boxes)], dtype='uint8')\n",
    "        \n",
    "        class_ids = list()\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "            class_ids.append(self.class_names.index('table'))\n",
    "        return masks, np.asarray(class_ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_dataset = TableBankDataset()\n",
    "train_dataset.load_dataset(dataset_dir='C:/Users/dsash/Repository/table/TableBank/Detection', is_train=True)\n",
    "train_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableBankConfig(mrcnn.config.Config):\n",
    "    NAME = \"mask_rcnn_tablebank_cfg\"\n",
    "\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    \n",
    "    NUM_CLASSES = 2\n",
    "\n",
    "    STEPS_PER_EPOCH = 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '%20%20%202013_2'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19484\\3890855023.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Train\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mtrain_dataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTableBankDataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mtrain_dataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'C:/Users/dsash/Repository/table/TableBank/Detection'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mis_train\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mtrain_dataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprepare\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19484\\3658597125.py\u001B[0m in \u001B[0;36mload_dataset\u001B[1;34m(self, dataset_dir, is_train)\u001B[0m\n\u001B[0;32m     11\u001B[0m             \u001B[0mimage_id\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m             \u001B[1;32mif\u001B[0m \u001B[0mis_train\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage_id\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;36m150\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m                 \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: invalid literal for int() with base 10: '%20%20%202013_2'"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_dataset = TableBankDataset()\n",
    "train_dataset.load_dataset(dataset_dir='C:/Users/dsash/Repository/table/TableBank/Detection', is_train=True)\n",
    "train_dataset.prepare()\n",
    "\n",
    "# Validation\n",
    "validation_dataset = TableBankDataset()\n",
    "validation_dataset.load_dataset(dataset_dir='C:/Users/dsash/Repository/table/TableBank/Detection', is_train=False)\n",
    "validation_dataset.prepare()\n",
    "\n",
    "# Model Configuration\n",
    "tablebank_config = TableBankConfig()\n",
    "\n",
    "# Build the Mask R-CNN Model Architecture\n",
    "model = mrcnn.model.MaskRCNN(mode='training', \n",
    "                             model_dir='./', \n",
    "                             config=tablebank_config)\n",
    "\n",
    "model.load_weights(filepath='mask_rcnn_coco.h5', \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "model.train(train_dataset=train_dataset, \n",
    "            val_dataset=validation_dataset, \n",
    "            learning_rate=tablebank_config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')\n",
    "\n",
    "model_path = 'tablebank_mask_rcnn_trained.h5'\n",
    "model.keras_model.save_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
